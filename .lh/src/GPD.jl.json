{
    "sourceFile": "src/GPD.jl",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 7,
            "patches": [
                {
                    "date": 1626310029646,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1626314400489,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,11 +1,11 @@\n using LinearAlgebra\n using LoopVectorization\n using Statistics\n+using TensorOperations\n using Tullio\n \n \n-\n \"\"\"\n     gpdfit(\n         sample::AbstractVector{T<:AbstractFloat}; \n         wip::Bool=true, \n"
                },
                {
                    "date": 1626314494491,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,17 +54,17 @@\n     θ_hats = similar(sample, grid_size)\n     ξ_hats = similar(sample, grid_size)\n     @turbo @. θ_hats =\n         inv(sample[len]) + (1 - sqrt((grid_size+1) / $(1:grid_size))) / prior / quartile\n-    @tullio threads=false ξ_hats[x] := log1p(-θ_hats[x] * sample[y]) |> _ / len\n+    @tullio avx=false tensor=true threads=false ξ_hats[x] := log1p(-θ_hats[x] * sample[y]) |> _ / len\n     log_like = similar(ξ_hats)\n     # Calculate profile log-likelihood at each estimate:\n     @turbo @. log_like = len * (log(-θ_hats / ξ_hats) - ξ_hats - 1)\n     # Calculate weights from log-likelihood:\n     weights = ξ_hats  # Reuse preallocated array (which is no longer in use)\n-    @tullio threads=false weights[y] = exp(log_like[x] - log_like[y]) |> inv\n+    @tullio avx=false tensor=true threads=false weights[y] = exp(log_like[x] - log_like[y]) |> inv\n     # Take weighted mean:\n-    @tullio threads=false θ_hat := weights[x] * θ_hats[x]\n+    @tullio avx=false tensor=true threads=false θ_hat := weights[x] * θ_hats[x]\n \n     ξ::T = calc_ξ(sample, θ_hat)\n     σ::T = -ξ / θ_hat\n \n"
                },
                {
                    "date": 1626314557716,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,11 +1,11 @@\n using LinearAlgebra\n using LoopVectorization\n using Statistics\n-using TensorOperations\n using Tullio\n \n \n+\n \"\"\"\n     gpdfit(\n         sample::AbstractVector{T<:AbstractFloat}; \n         wip::Bool=true, \n@@ -54,17 +54,17 @@\n     θ_hats = similar(sample, grid_size)\n     ξ_hats = similar(sample, grid_size)\n     @turbo @. θ_hats =\n         inv(sample[len]) + (1 - sqrt((grid_size+1) / $(1:grid_size))) / prior / quartile\n-    @tullio avx=false tensor=true threads=false ξ_hats[x] := log1p(-θ_hats[x] * sample[y]) |> _ / len\n+    @tullio threads=false ξ_hats[x] := log1p(-θ_hats[x] * sample[y]) |> _ / len\n     log_like = similar(ξ_hats)\n     # Calculate profile log-likelihood at each estimate:\n     @turbo @. log_like = len * (log(-θ_hats / ξ_hats) - ξ_hats - 1)\n     # Calculate weights from log-likelihood:\n     weights = ξ_hats  # Reuse preallocated array (which is no longer in use)\n-    @tullio avx=false tensor=true threads=false weights[y] = exp(log_like[x] - log_like[y]) |> inv\n+    @tullio threads=false weights[y] = exp(log_like[x] - log_like[y]) |> inv\n     # Take weighted mean:\n-    @tullio avx=false tensor=true threads=false θ_hat := weights[x] * θ_hats[x]\n+    @tullio threads=false θ_hat := weights[x] * θ_hats[x]\n \n     ξ::T = calc_ξ(sample, θ_hat)\n     σ::T = -ξ / θ_hat\n \n"
                },
                {
                    "date": 1626314693762,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,8 @@\n using Statistics\n using Tullio\n \n \n-\n \"\"\"\n     gpdfit(\n         sample::AbstractVector{T<:AbstractFloat}; \n         wip::Bool=true, \n@@ -52,19 +51,19 @@\n \n     # build pointwise estimates of ξ and θ at each grid point\n     θ_hats = similar(sample, grid_size)\n     ξ_hats = similar(sample, grid_size)\n-    @turbo @. θ_hats =\n+    @tturbo @. θ_hats =\n         inv(sample[len]) + (1 - sqrt((grid_size+1) / $(1:grid_size))) / prior / quartile\n-    @tullio threads=false ξ_hats[x] := log1p(-θ_hats[x] * sample[y]) |> _ / len\n+    @tullio ξ_hats[x] := log1p(-θ_hats[x] * sample[y]) |> _ / len\n     log_like = similar(ξ_hats)\n     # Calculate profile log-likelihood at each estimate:\n-    @turbo @. log_like = len * (log(-θ_hats / ξ_hats) - ξ_hats - 1)\n+    @tturbo @. log_like = len * (log(-θ_hats / ξ_hats) - ξ_hats - 1)\n     # Calculate weights from log-likelihood:\n     weights = ξ_hats  # Reuse preallocated array (which is no longer in use)\n-    @tullio threads=false weights[y] = exp(log_like[x] - log_like[y]) |> inv\n+    @tullio weights[y] = exp(log_like[x] - log_like[y]) |> inv\n     # Take weighted mean:\n-    @tullio threads=false θ_hat := weights[x] * θ_hats[x]\n+    @tullio θ_hat := weights[x] * θ_hats[x]\n \n     ξ::T = calc_ξ(sample, θ_hat)\n     σ::T = -ξ / θ_hat\n \n@@ -103,9 +102,9 @@\n Calculate ξ, the parameter for the GPD.\n \"\"\"\n function calc_ξ(sample::AbstractVector{T}, θ_hat::T) where {T<:AbstractFloat}\n     ξ = zero(T)\n-    @turbo for i in eachindex(sample)\n+    @tturbo for i in eachindex(sample)\n         ξ += log1p(-θ_hat * sample[i]) / length(sample)\n     end\n     return ξ::T\n end\n"
                },
                {
                    "date": 1626314740174,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,19 +51,19 @@\n \n     # build pointwise estimates of ξ and θ at each grid point\n     θ_hats = similar(sample, grid_size)\n     ξ_hats = similar(sample, grid_size)\n-    @tturbo @. θ_hats =\n+    @turbo @. θ_hats =\n         inv(sample[len]) + (1 - sqrt((grid_size+1) / $(1:grid_size))) / prior / quartile\n-    @tullio ξ_hats[x] := log1p(-θ_hats[x] * sample[y]) |> _ / len\n+    @tullio threads=false ξ_hats[x] := log1p(-θ_hats[x] * sample[y]) |> _ / len\n     log_like = similar(ξ_hats)\n     # Calculate profile log-likelihood at each estimate:\n-    @tturbo @. log_like = len * (log(-θ_hats / ξ_hats) - ξ_hats - 1)\n+    @turbo @. log_like = len * (log(-θ_hats / ξ_hats) - ξ_hats - 1)\n     # Calculate weights from log-likelihood:\n     weights = ξ_hats  # Reuse preallocated array (which is no longer in use)\n-    @tullio weights[y] = exp(log_like[x] - log_like[y]) |> inv\n+    @tullio threads=false weights[y] = exp(log_like[x] - log_like[y]) |> inv\n     # Take weighted mean:\n-    @tullio θ_hat := weights[x] * θ_hats[x]\n+    @tullio threads=false θ_hat := weights[x] * θ_hats[x]\n \n     ξ::T = calc_ξ(sample, θ_hat)\n     σ::T = -ξ / θ_hat\n \n@@ -102,9 +102,9 @@\n Calculate ξ, the parameter for the GPD.\n \"\"\"\n function calc_ξ(sample::AbstractVector{T}, θ_hat::T) where {T<:AbstractFloat}\n     ξ = zero(T)\n-    @tturbo for i in eachindex(sample)\n+    @turbo for i in eachindex(sample)\n         ξ += log1p(-θ_hat * sample[i]) / length(sample)\n     end\n     return ξ::T\n end\n"
                },
                {
                    "date": 1626362460303,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,8 +44,9 @@\n \n \n     prior = 3\n     grid_size = min_grid_pts + isqrt(len) # isqrt = floor sqrt\n+    grid_size = grid_size - (grid_size % 32) # multiples of 32 speed up calcs\n     n_0 = 10  # determines how strongly to nudge ξ towards .5\n     quartile::T = sample[(len+2)÷4]\n \n \n"
                },
                {
                    "date": 1626362509519,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,9 +44,8 @@\n \n \n     prior = 3\n     grid_size = min_grid_pts + isqrt(len) # isqrt = floor sqrt\n-    grid_size = grid_size - (grid_size % 32) # multiples of 32 speed up calcs\n     n_0 = 10  # determines how strongly to nudge ξ towards .5\n     quartile::T = sample[(len+2)÷4]\n \n \n"
                }
            ],
            "date": 1626310029646,
            "name": "Commit-0",
            "content": "using LinearAlgebra\nusing LoopVectorization\nusing Statistics\nusing Tullio\n\n\n\n\"\"\"\n    gpdfit(\n        sample::AbstractVector{T<:AbstractFloat}; \n        wip::Bool=true, \n        min_grid_pts::Integer=30, \n        sort_sample::Bool=false\n        ) -> (ξ::T, σ::T)\n\nReturn a named list of estimates for the parameters ξ (shape) and σ (scale) of the\ngeneralized Pareto distribution (GPD), assuming the location parameter is 0.\n\n# Arguments\n\n  - `sample::AbstractVector`: A numeric vector. The sample from which to estimate\n    the parameters.\n  - `wip::Bool = true`: Logical indicating whether to adjust ξ based on a weakly informative\n    Gaussian prior centered on 0.5. Defaults to `true`.\n  - `min_grid_pts::Integer = 30`: The minimum number of grid points used in the fitting\n    algorithm. The actual number used is `min_grid_pts + ⌊sqrt(length(sample))⌋`.\n\n# Note\n\nEstimation method taken from Zhang, J. and Stephens, M.A. (2009). The parameter ξ is the\nnegative of \\$k\\$.\n\"\"\"\nfunction gpdfit(\n    sample::AbstractVector{T};\n    wip::Bool=true,\n    min_grid_pts::Integer=30,\n    sort_sample::Bool=false,\n) where {T<:AbstractFloat}\n\n    len = length(sample)\n    # sample must be sorted, but we can skip if sample is already sorted\n    if sort_sample\n        sample = sort(sample; alg=QuickSort)\n    end\n\n\n    prior = 3\n    grid_size = min_grid_pts + isqrt(len) # isqrt = floor sqrt\n    n_0 = 10  # determines how strongly to nudge ξ towards .5\n    quartile::T = sample[(len+2)÷4]\n\n\n    # build pointwise estimates of ξ and θ at each grid point\n    θ_hats = similar(sample, grid_size)\n    ξ_hats = similar(sample, grid_size)\n    @turbo @. θ_hats =\n        inv(sample[len]) + (1 - sqrt((grid_size+1) / $(1:grid_size))) / prior / quartile\n    @tullio threads=false ξ_hats[x] := log1p(-θ_hats[x] * sample[y]) |> _ / len\n    log_like = similar(ξ_hats)\n    # Calculate profile log-likelihood at each estimate:\n    @turbo @. log_like = len * (log(-θ_hats / ξ_hats) - ξ_hats - 1)\n    # Calculate weights from log-likelihood:\n    weights = ξ_hats  # Reuse preallocated array (which is no longer in use)\n    @tullio threads=false weights[y] = exp(log_like[x] - log_like[y]) |> inv\n    # Take weighted mean:\n    @tullio threads=false θ_hat := weights[x] * θ_hats[x]\n\n    ξ::T = calc_ξ(sample, θ_hat)\n    σ::T = -ξ / θ_hat\n\n    # Drag towards .5 to reduce variance for small len\n    if wip\n        ξ = (ξ * len + 0.5 * n_0) / (len + n_0)\n    end\n\n    return ξ::T, σ::T\n\nend\n\n\"\"\"\n    gpd_quantile(p::T, k::T, sigma::T) where {T<:AbstractFloat} -> T\n\nCompute the `p` quantile of the Generalized Pareto Distribution (GPD).\n\n# Arguments\n\n  - `p`: A scalar between 0 and 1.\n  - `ξ`: A scalar shape parameter.\n  - `σ`: A scalar scale parameter.\n\n# Returns\n\nA quantile of the Generalized Pareto Distribution.\n\"\"\"\nfunction gpd_quantile(p, k::T, sigma::T) where {T<:AbstractFloat}\n    return sigma * expm1(-k * log1p(-p)) / k\nend\n\n\n\"\"\"\n    calc_ξ(sample, θ_hat)\n\nCalculate ξ, the parameter for the GPD.\n\"\"\"\nfunction calc_ξ(sample::AbstractVector{T}, θ_hat::T) where {T<:AbstractFloat}\n    ξ = zero(T)\n    @turbo for i in eachindex(sample)\n        ξ += log1p(-θ_hat * sample[i]) / length(sample)\n    end\n    return ξ::T\nend\n\n"
        }
    ]
}